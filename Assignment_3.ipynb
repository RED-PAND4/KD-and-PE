{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548fcc69",
   "metadata": {},
   "source": [
    "# Analysis of Paper Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0e2a5",
   "metadata": {},
   "source": [
    "In this assignment, I chose to use both trigrams and embeddings to analyze the reviews of rejected papers because they offer complementary strengths in understanding the textual data. The goal of this analysis is to identify recurring themes and reasons for rejection.\n",
    "\n",
    "Trigrams are particularly useful because they capture sequences of three consecutive words, which provide more context than individual words. For example, a trigram like \"lack of clarity\" is much more informative than just \"lack\" or \"clarity\" on their own. By extracting trigrams that include negative keywords such as \"poor,\" \"weak,\" or \"insufficient,\" I can focus on phrases that are likely to highlight specific problems in the papers.\n",
    "\n",
    "However, relying solely on trigrams has its limitations. Trigrams are based on exact word matches, so similar phrases with different wording, like \"poor clarity\" and \"unclear explanation,\" would not be grouped together. This is where embeddings come into play. Embeddings, generated using a pre-trained model like Sentence Transformers, encode the semantic meaning of words or phrases into numerical vectors. These vectors allow me to measure the similarity between phrases based on their meaning rather than their exact wording. By grouping trigrams with similar embeddings, I can cluster phrases that express the same idea, even if they use different words. This helps to reduce redundancy and ensures that I capture broader themes in the reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7cdc9",
   "metadata": {},
   "source": [
    "## *Common*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12240c3",
   "metadata": {},
   "source": [
    "I start with importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, trigrams, ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbcc04",
   "metadata": {},
   "source": [
    "I have started by importing the data and filtering the review by only the paper that where rejected since these review will contains reasons that explain why they weren't accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Filter for Rejected Papers ---\n",
    "# Filter papers with \"Decision:###Reject\"\n",
    "data = pd.read_excel('tp_2020conference.xlsx')\n",
    "rejected_papers = data[data['paper_decision'].str.contains(\"Decision:###Reject\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284674d",
   "metadata": {},
   "source": [
    "I have clean and standardized the text by lowercasing, removing numbers, special characters and stopwords to have a consistent format. The I have tokenized them into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac12905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Preprocessing and Tokenization ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)  # Remove text inside parentheses\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'\\b(weak|reject|accept|e|g)\\b', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "rejected_papers['clean_review'] = rejected_papers['review'].apply(clean_text).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c13d4",
   "metadata": {},
   "source": [
    "I have extracted trigrams from the cleaned text that contained common negative keywords that often indicate reason for rejection. The trigrams should provide the context around these keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Extract Trigrams ---\n",
    "negative_keywords = {\"lack\", \"poor\", \"unclear\", \"weak\", \"missing\", \"limited\", \"no\", \"incomplete\", \"insufficient\"}\n",
    "\n",
    "trigrams_list = []\n",
    "\n",
    "for review in rejected_papers['clean_review']:\n",
    "    sentences = nltk.sent_tokenize(review)  # Split review into sentences\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Generate trigrams\n",
    "        trigrams_list.extend([trigram for trigram in nltk.trigrams(tokens) if any(word in negative_keywords for word in trigram)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c717b",
   "metadata": {},
   "source": [
    "## *Version 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d613d",
   "metadata": {},
   "source": [
    "In this first version I try to calculate the frequency distribution of the trigrams but this have a poor result since it do not consider trigrams with same words but different order the same trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79932103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Frequency Distribution ---\n",
    "trigram_freq = FreqDist(trigrams_list)\n",
    "# Get the most common trigrams\n",
    "most_common_trigrams = trigram_freq.most_common(20)\n",
    "\n",
    "\n",
    "# --- Cell: Visualization ---\n",
    "# Plot most common trigrams\n",
    "plt.figure(figsize=(10, 5))\n",
    "trigrams_labels = [' '.join(trigram) for trigram, _ in most_common_trigrams]\n",
    "trigrams_counts = [count for _, count in most_common_trigrams]\n",
    "plt.barh(trigrams_labels, trigrams_counts, color='lightcoral')\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Trigrams\")\n",
    "plt.title(\"Most Common Negative Sentiment Trigrams with Keywords\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- Cell: Insights ---\n",
    "# Print the most common trigrams\n",
    "print(\"Most Common Negative Sentiment Trigrams with Keywords:\")\n",
    "for trigram, count in most_common_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dca84",
   "metadata": {},
   "source": [
    "So I have tried to group the trigrams in this way: if two of the words present in the trigrams are same they are putted together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e295d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Group Trigrams by Similarity ---\n",
    "def group_trigrams_by_similarity(trigrams):\n",
    "    \"\"\"\n",
    "    Group trigrams that have at least two identical words.\n",
    "    \"\"\"\n",
    "    grouped_trigrams = defaultdict(list)\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        # Generate all combinations of two words from the trigram\n",
    "        two_word_combinations = list(combinations(trigram, 2))\n",
    "\n",
    "        # Use the two-word combination as a key to group similar trigrams\n",
    "        for combo in two_word_combinations:\n",
    "            grouped_trigrams[combo].append(trigram)\n",
    "\n",
    "    # Filter groups to keep only those with more than one trigram\n",
    "    grouped_trigrams = {key: value for key, value in grouped_trigrams.items() if len(value) > 1}\n",
    "\n",
    "    return grouped_trigrams\n",
    "\n",
    "grouped_trigrams = group_trigrams_by_similarity(trigrams_list)\n",
    "\n",
    "# Count occurrences of each group\n",
    "grouped_trigram_counts = {key: len(value) for key, value in grouped_trigrams.items()}\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_grouped_trigrams = sorted(grouped_trigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --- Cell: Visualization ---\n",
    "# Plot most common grouped trigrams\n",
    "plt.figure(figsize=(10, 5))\n",
    "grouped_trigrams_labels = ['  '.join(key) for key, _ in sorted_grouped_trigrams[:20]]\n",
    "grouped_trigrams_counts = [count for _, count in sorted_grouped_trigrams[:20]]\n",
    "plt.barh(grouped_trigrams_labels, grouped_trigrams_counts, color='lightcoral')\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Grouped Trigrams\")\n",
    "plt.title(\"Most Common Grouped Trigrams with Similar Words\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --- Cell: Insights ---\n",
    "# Print the most common grouped trigrams\n",
    "print(\"Most Common Grouped Trigrams with Similar Words:\")\n",
    "for trigram, count in sorted_grouped_trigrams[:20]:\n",
    "    print(f\"{' & '.join(trigram)}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022aa9d5",
   "metadata": {},
   "source": [
    "Also this method produced poor result so I developed the Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0247960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE CODE 1.1\n",
    "# --- Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, trigrams, ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Cell: Filter for Rejected Papers ---\n",
    "# Filter papers with \"Decision:###Reject\"\n",
    "data = pd.read_excel('tp_2020conference.xlsx')\n",
    "rejected_papers = data[data['paper_decision'].str.contains(\"Decision:###Reject\", na=False)]\n",
    "\n",
    "# --- Cell: Preprocessing and Tokenization ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)  # Remove text inside parentheses\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'\\b(weak|reject|accept|e|g)\\b', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "rejected_papers['clean_review'] = rejected_papers['review'].apply(clean_text).apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# --- Cell: Extract Trigrams \n",
    "negative_keywords = {\"lack\", \"poor\", \"unclear\", \"weak\", \"missing\", \"limited\", \"no\", \"incomplete\", \"insufficient\"}\n",
    "\n",
    "trigrams_list = []\n",
    "\n",
    "for review in rejected_papers['clean_review']:\n",
    "    sentences = nltk.sent_tokenize(review)  # Split review into sentences\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Generate trigrams \n",
    "        trigrams_list.extend([trigram for trigram in nltk.trigrams(tokens) if any(word in negative_keywords for word in trigram)])\n",
    "\n",
    "# --- Cell: Frequency Distribution ---\n",
    "trigram_freq = FreqDist(trigrams_list)\n",
    "\n",
    "\n",
    "# Get the most common trigrams\n",
    "most_common_trigrams = trigram_freq.most_common(20)\n",
    "\n",
    "\n",
    "# --- Cell: Visualization ---\n",
    "# Plot most common trigrams\n",
    "plt.figure(figsize=(10, 5))\n",
    "trigrams_labels = [' '.join(trigram) for trigram, _ in most_common_trigrams]\n",
    "trigrams_counts = [count for _, count in most_common_trigrams]\n",
    "plt.barh(trigrams_labels, trigrams_counts, color='lightcoral')\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Trigrams\")\n",
    "plt.title(\"Most Common Negative Sentiment Trigrams with Keywords\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- Cell: Insights ---\n",
    "# Print the most common trigrams\n",
    "print(\"Most Common Negative Sentiment Trigrams with Keywords:\")\n",
    "for trigram, count in most_common_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE CODE 1.2\n",
    "# --- Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, trigrams, ngrams\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Cell: Filter for Rejected Papers ---\n",
    "# Filter papers with \"Decision:###Reject\"\n",
    "data = pd.read_excel('tp_2020conference.xlsx')\n",
    "rejected_papers = data[data['paper_decision'].str.contains(\"Decision:###Reject\", na=False)]\n",
    "\n",
    "# --- Cell: Sentiment Analysis for Rejected Papers ---\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "rejected_papers['review_sentiment'] = rejected_papers['review'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# --- Cell: Preprocessing and Tokenization ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)  # Remove text inside parentheses\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'\\b(weak|reject|accept|e|g)\\b', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "rejected_papers['clean_review'] = rejected_papers['review'].apply(clean_text).apply(remove_stopwords)\n",
    "\n",
    "def group_trigrams_by_similarity(trigrams):\n",
    "    \"\"\"\n",
    "    Group trigrams that have at least two identical words.\n",
    "    \"\"\"\n",
    "    grouped_trigrams = defaultdict(list)\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        # Generate all combinations of two words from the trigram\n",
    "        two_word_combinations = list(combinations(trigram, 2))\n",
    "\n",
    "        # Use the two-word combination as a key to group similar trigrams\n",
    "        for combo in two_word_combinations:\n",
    "            grouped_trigrams[combo].append(trigram)\n",
    "\n",
    "    # Filter groups to keep only those with more than one trigram\n",
    "    grouped_trigrams = {key: value for key, value in grouped_trigrams.items() if len(value) > 1}\n",
    "\n",
    "    return grouped_trigrams\n",
    "\n",
    "# --- Cell: Extract Trigrams ---\n",
    "negative_keywords = {\"lack\", \"poor\", \"unclear\", \"weak\", \"missing\", \"limited\", \"no\", \"incomplete\", \"insufficient\"}\n",
    "\n",
    "trigrams_list = []\n",
    "\n",
    "for review in rejected_papers['clean_review']:\n",
    "    sentences = nltk.sent_tokenize(review)  # Split review into sentences\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Generate trigrams\n",
    "        trigrams_list.extend([trigram for trigram in nltk.trigrams(tokens) if any(word in negative_keywords for word in trigram)])\n",
    "\n",
    "# --- Cell: Group Trigrams by Similarity ---\n",
    "grouped_trigrams = group_trigrams_by_similarity(trigrams_list)\n",
    "\n",
    "# Count occurrences of each group\n",
    "grouped_trigram_counts = {key: len(value) for key, value in grouped_trigrams.items()}\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_grouped_trigrams = sorted(grouped_trigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --- Cell: Visualization ---\n",
    "# Plot most common grouped trigrams\n",
    "plt.figure(figsize=(10, 5))\n",
    "grouped_trigrams_labels = [' & '.join(key) for key, _ in sorted_grouped_trigrams[:20]]\n",
    "grouped_trigrams_counts = [count for _, count in sorted_grouped_trigrams[:20]]\n",
    "plt.barh(grouped_trigrams_labels, grouped_trigrams_counts, color='lightcoral')\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Grouped Trigrams\")\n",
    "plt.title(\"Most Common Grouped Trigrams with Similar Words\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --- Cell: Insights ---\n",
    "# Print the most common grouped trigrams\n",
    "print(\"Most Common Grouped Trigrams with Similar Words:\")\n",
    "for trigram, count in sorted_grouped_trigrams[:20]:\n",
    "    print(f\"{' & '.join(trigram)}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074fa7c",
   "metadata": {},
   "source": [
    "## *Version 2 with embeddings*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d71939",
   "metadata": {},
   "source": [
    "In this version I try to group the trigrams by semantic similarity through an embedding model and cosine similarity, so they should be grouped by common themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed829a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: Trigram Embedding and Grouping ---\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_trigram_embedding(trigram):\n",
    "    word_embeddings = [embedding_model.encode(word) for word in trigram]\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "def group_trigrams_by_embedding(trigrams, threshold=0.8):\n",
    "    trigram_embeddings = {trigram: get_trigram_embedding(trigram) for trigram in trigrams}\n",
    "    grouped_trigrams = defaultdict(list)\n",
    "    visited = set()\n",
    "\n",
    "    for trigram1, embedding1 in trigram_embeddings.items():\n",
    "        if trigram1 in visited:\n",
    "            continue\n",
    "        group = [trigram1]\n",
    "        visited.add(trigram1)\n",
    "        for trigram2, embedding2 in trigram_embeddings.items():\n",
    "            if trigram2 in visited:\n",
    "                continue\n",
    "            similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "            if similarity >= threshold:\n",
    "                group.append(trigram2)\n",
    "                visited.add(trigram2)\n",
    "        grouped_trigrams[tuple(group)].append(group)\n",
    "\n",
    "    return grouped_trigrams\n",
    "\n",
    "grouped_trigrams = group_trigrams_by_embedding(trigrams_list, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3f3a4",
   "metadata": {},
   "source": [
    "The I have saved the result in a csv file for a more easy use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a228c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell: CSV Output ---\n",
    "def save_grouped_trigrams_with_first_to_csv(grouped_trigrams, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"First Trigram\", \"Grouped Trigrams\"])\n",
    "        for group in grouped_trigrams.items():\n",
    "            trigrams_text = ' | '.join([' '.join(trigram) for trigram in group])\n",
    "            first_trigram = ' '.join(group[0])\n",
    "            writer.writerow([first_trigram,  trigrams_text])\n",
    "\n",
    "output_file = \"grouped_trigrams_with_first.csv\"\n",
    "save_grouped_trigrams_with_first_to_csv(grouped_trigrams, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE CODE\n",
    "# --- Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# --- Cell: Filter for Rejected Papers ---\n",
    "# Filter papers with \"Decision:###Reject\"\n",
    "data = pd.read_excel('tp_2020conference.xlsx')\n",
    "rejected_papers = data[data['paper_decision'].str.contains(\"Decision:###Reject\", na=False)]\n",
    "\n",
    "\n",
    "# --- Cell: Preprocessing and Tokenization ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)  # Remove text inside parentheses\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'\\b(weak|reject|accept|e|g)\\b', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "rejected_papers['clean_review'] = rejected_papers['review'].apply(clean_text).apply(remove_stopwords)\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use other models like 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "def get_trigram_embedding(trigram):\n",
    "    \"\"\"\n",
    "    Compute the embedding for a trigram by averaging the embeddings of its words.\n",
    "    \"\"\"\n",
    "    word_embeddings = [embedding_model.encode(word) for word in trigram]\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "def group_trigrams_by_embedding(trigrams, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Group trigrams based on cosine similarity of their embeddings.\n",
    "    \"\"\"\n",
    "    trigram_embeddings = {trigram: get_trigram_embedding(trigram) for trigram in trigrams}\n",
    "    grouped_trigrams = defaultdict(list)\n",
    "    visited = set()\n",
    "\n",
    "    for trigram1, embedding1 in trigram_embeddings.items():\n",
    "        if trigram1 in visited:\n",
    "            continue\n",
    "        group = [trigram1]\n",
    "        visited.add(trigram1)\n",
    "        for trigram2, embedding2 in trigram_embeddings.items():\n",
    "            if trigram2 in visited:\n",
    "                continue\n",
    "            similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "            if similarity >= threshold:\n",
    "                group.append(trigram2)\n",
    "                visited.add(trigram2)\n",
    "        grouped_trigrams[tuple(group)].append(group)\n",
    "\n",
    "    return grouped_trigrams\n",
    "\n",
    "# --- Cell: Save Grouped Trigrams with First Trigram and Counts to CSV ---\n",
    "def save_grouped_trigrams_with_first_to_csv(grouped_trigrams, grouped_trigram_counts, output_file):\n",
    "    \"\"\"\n",
    "    Save grouped trigrams to a CSV file with the first trigram for each group.\n",
    "    \"\"\"\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"First Trigram\", \"Grouped Trigrams\"])  # Header\n",
    "        for group in grouped_trigrams.items():\n",
    "            # Join trigrams in the group with \" | \"\n",
    "            trigrams_text = ' | '.join([' '.join(trigram) for trigram in group])\n",
    "            # Use the first trigram of the group\n",
    "            first_trigram = ' '.join(group[0])\n",
    "            # Write the trigrams, count, first trigram, and grouping criterion to the CSV\n",
    "            writer.writerow([first_trigram, grouped_trigram_counts, trigrams_text])\n",
    "\n",
    "\n",
    "# --- Cell: Extract Trigrams ---\n",
    "negative_keywords = {\"lack\", \"poor\", \"unclear\", \"weak\", \"missing\", \"limited\", \"no\", \"incomplete\", \"insufficient\"}\n",
    "\n",
    "trigrams_list = []\n",
    "\n",
    "for review in rejected_papers['clean_review']:\n",
    "    sentences = nltk.sent_tokenize(review)  # Split review into sentences\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Generate trigrams\n",
    "        trigrams_list.extend([trigram for trigram in nltk.trigrams(tokens) if any(word in negative_keywords for word in trigram)])\n",
    "\n",
    "# --- Cell: Group Trigrams by Embedding ---\n",
    "grouped_trigrams = group_trigrams_by_embedding(trigrams_list, threshold=0.8)\n",
    "\n",
    "# Count occurrences of each group\n",
    "grouped_trigram_counts = {key: len(value) for key, value in grouped_trigrams.items()}\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_grouped_trigrams = sorted(grouped_trigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# --- Cell: Insights ---\n",
    "# Print the most common grouped trigrams\n",
    "print(\"Most Common Grouped Trigrams (Using Embeddings):\")\n",
    "for group, count in sorted_grouped_trigrams[:20]:\n",
    "    print(f\"{' | '.join([' '.join(trigram) for trigram in group])}: {count}\")\n",
    "\n",
    "# --- Cell: Save Grouped Trigrams with First Trigram and Counts to CSV ---\n",
    "# Specify the output file path\n",
    "output_file = \"grouped_trigrams_with_first.csv\"\n",
    "\n",
    "# Save the grouped trigrams with the first trigram and details to the CSV file\n",
    "save_grouped_trigrams_with_first_to_csv(grouped_trigrams, grouped_trigram_counts, output_file)\n",
    "print(f\"Grouped trigrams with the first trigram and details have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1cd9d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Combining trigrams and embeddings creates a balanced approach. Trigrams provide interpretable, context-rich phrases that directly point to specific issues, while embeddings enable me to generalize and group these issues into meaningful clusters. For example, trigrams like \"poor clarity,\" \"unclear explanation,\" and \"lack of detail\" might all be grouped under a broader theme of \"clarity issues.\" This dual approach ensures that my analysis captures both the granular details and the overarching patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de170875",
   "metadata": {},
   "source": [
    "*Suggestion: Copy the Whole code in Google colab*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
