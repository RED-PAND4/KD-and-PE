{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635a382e",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "\n",
    "# The Midnight Mystery: A Data Mining Murder Investigation\n",
    "\n",
    "## Background\n",
    "\n",
    "It's a dark and stormy night at the Grand Hotel. Many distinguished guests have gathered for an exclusive gala. At precisely 11:50 PM, a scream pierces the night - one guest has been murdered!\n",
    "\n",
    "The legendary detective Sherlock Holmes and his assistant Dr. Watson arrive at the scene. They conduct thorough interviews with all surviving guests. Each guest provides information about their whereabouts at the time of the murder and whom they remember seeing.\n",
    "\n",
    "A crucial piece of evidence is found: a handwritten note clutched in the victim's hand, apparently torn from the murderer during the struggle. The note appears to be written in a distinctive style.\n",
    "\n",
    "Your task is to help Holmes and Watson identify the most likely suspects.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "You are provided with `murder_mystery.json` containing:\n",
    "- **Metadata**: Case details including victim name, murder time, and the mysterious note\n",
    "- **Interrogations**: Interview reports, each containing:\n",
    "  - Guest name\n",
    "  - Their statement about location and sightings\n",
    "  - Interview timestamp\n",
    "\n",
    "## Your Mission\n",
    "\n",
    "You must analyze the evidence to identify the prime suspects. Your investigation should follow these steps:\n",
    "\n",
    "\n",
    "Combine your findings from all analyses to:\n",
    "1. Identify the most likely murderer\n",
    "2. Provide evidence supporting your conclusion\n",
    "3. Explain any alternative suspects and why they were ruled out\n",
    "\n",
    "**Deliverable**: A final report presenting your conclusion with supporting evidence, and the code.\n",
    "\n",
    "## Technical Requirements\n",
    "\n",
    "- You can use any Python  libraries you like, such as: `networkx`, `python-louvain`, `sentence-transformers`, `openai/anthropic` (via OpenRouter)\n",
    "- Your code should be well-documented and reproducible\n",
    "- Handle API rate limits appropriately\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "- **Code Quality**: Clean, documented, efficient code\n",
    "- **Analysis Depth**: Thoroughness of investigation and use of multiple techniques\n",
    "- **Conclusions**: Logical reasoning and evidence-based conclusions\n",
    "\n",
    "## Hints\n",
    "\n",
    "- People may be mistaken as to who they saw. This is less likely so, if, for example, they were playing cards with a guest, rather than just briefly seeing them pass by.\n",
    "- Some guests might naturally be more isolated due to their behavior\n",
    "- Writing styles can reveal more than just dialect - look for patterns\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit a Jupyter notebook containing:\n",
    "1. All code with explanations\n",
    "2. Your final report as a markdown cell\n",
    "\n",
    "USE ONLY THE API KEY PROVIDED VIA EMAIL \n",
    "\n",
    "Good luck, detective! The truth is hidden in the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa63092",
   "metadata": {},
   "source": [
    "# Reasoning\n",
    "\n",
    "To identify the murderer, I approach the problem using a network analysis and natural language processing pipeline:\n",
    "\n",
    "1. **Graph Construction**\n",
    "   I begin by constructing a network graph where each node represents a person, and each edge (or link) represents a sighting or interaction between two individuals. I assign a **reliability score** to each edge based on the nature of the interaction:\n",
    "\n",
    "   * **1.0** if the two individuals directly interacted (e.g., had a conversation).\n",
    "   * **0.5** if one person merely observed the other.\n",
    "     These scores are determined with the help of OpenAI by analyzing the nature of the statements.\n",
    "\n",
    "2. **Filtering Based on Alibis**\n",
    "   Using this graph, I remove all individuals who had **at least one connection with a reliability score of 1.0** during the time of the murder. This implies they were with someone else and thus likely have a credible alibi.\n",
    "\n",
    "3. **Text Embedding and Similarity Matching**\n",
    "   For the remaining suspects, I embed both their statements and the mysterious note using **SentenceTransformer**.\n",
    "   I then calculate **cosine similarity** between each person's statement and the note to find semantic overlaps that might indicate authorship or involvement.\n",
    "\n",
    "4. **Narrowing Down Suspects**\n",
    "   Based on similarity scores, I shortlist the **top 3 suspects** whose statements most closely matched the note.\n",
    "\n",
    "5. **Language and Contextual Analysis with GPT**\n",
    "   I then provid these top 3 statements to ChatGPT along with the content of the note. ChatGPT analyze the writing style, tone, and potential motivations to suggest the most likely author of the note â€” and, by extension, the probable culprit.\n",
    "\n",
    "6. **Final Judgment Criteria**\n",
    "   The final determination is made using a combination of:\n",
    "\n",
    "   * **Alibi validation** (who was alone and unaccounted for),\n",
    "   * **Writing style similarity** between the note and statements,\n",
    "   * **Intent**, as interpreted from their statements by ChatGPT.\n",
    "\n",
    "In the following code I will try to execute this list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc8c38",
   "metadata": {},
   "source": [
    "## Code 0\n",
    "\n",
    "this is the first try, the following are trying to improve the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base code\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data\n",
    "with open('murder_mystery.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "metadata = data['metadata']\n",
    "interrogations = data['interrogations']\n",
    "\n",
    "# Extract the victim's note\n",
    "victim_note = metadata['victim_note']\n",
    "\n",
    "# Initialize the sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute the embedding for the victim's note\n",
    "victim_note_embedding = model.encode(victim_note, convert_to_tensor=True)\n",
    "\n",
    "# Analyze guest statements for similarity to the victim's note\n",
    "similarities = []\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "    statement_embedding = model.encode(statement, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(victim_note_embedding, statement_embedding).item()\n",
    "    similarities.append((guest, similarity))\n",
    "\n",
    "# Sort guests by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 3 most similar guests\n",
    "print(\"Top 3 guests with similar writing style to the victim's note:\")\n",
    "for guest, similarity in similarities[:3]:\n",
    "    print(f\"{guest}: Similarity = {similarity:.4f}\")\n",
    "\n",
    "\n",
    "# Build a network graph of guest interactions\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (guests)\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    G.add_node(guest)\n",
    "\n",
    "# Add edges based on sightings in statements\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "    # Extract mentioned guests (simplified heuristic)\n",
    "    mentioned_guests = [g for g in G.nodes if g in statement]\n",
    "    for mentioned_guest in mentioned_guests:\n",
    "        G.add_edge(guest, mentioned_guest)\n",
    "\n",
    "# Analyze the graph\n",
    "print(\"\\nNetwork Graph Analysis:\")\n",
    "print(f\"Number of nodes (guests): {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges (interactions): {G.number_of_edges()}\")\n",
    "\n",
    "# Identify isolated nodes (guests with no interactions)\n",
    "isolated_guests = list(nx.isolates(G))\n",
    "print(f\"Isolated guests: {isolated_guests}\")\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=10)\n",
    "plt.title(\"Guest Interaction Network\")\n",
    "plt.show()\n",
    "\n",
    "# Combine results to identify the murderer\n",
    "prime_suspect = similarities[0][0]  # Guest with the highest similarity to the victim's note\n",
    "print(f\"\\nPrime Suspect based on writing style: {prime_suspect}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e808a2",
   "metadata": {},
   "source": [
    "# Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d978154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code with chatgpt prompt\n",
    "import json\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up OpenAI API key\n",
    "#openai.api_key = \"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\"\n",
    "client = OpenAI(api_key=\"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\",\n",
    "base_url=\"https://openrouter.ai/api/v1\")\n",
    "\n",
    "# Load the data\n",
    "with open('murder_mystery.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "metadata = data['metadata']\n",
    "interrogations = data['interrogations']\n",
    "\n",
    "# Extract the victim's note\n",
    "victim_note = metadata['victim_note']\n",
    "\n",
    "# Initialize the sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to query OpenAI for interaction reliability\n",
    "def ask_openai_for_reliability(guest, statement):\n",
    "    prompt = f\"\"\"\n",
    "    The following is a statement from {guest}:\n",
    "    \"{statement}\"\n",
    "    \n",
    "    Please extract two lists:\n",
    "    1. Guests the person has interacted with (e.g., playing cards, having a drink, chatting).\n",
    "    2. Guests the person has only seen (e.g., be seen, passing by or observing).\n",
    "    \n",
    "    Return the lists in JSON format with keys \"interacted_with\" and \"only_seen\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                model=\"openai/gpt-4.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a precise information extraction assistant. Extract only what is explicitly stated.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "        return {\"interacted_with\": [], \"only_seen\": []}\n",
    "\n",
    "# Build a network graph of guest interactions with reliability parameter\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (guests)\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    G.add_node(guest)\n",
    "\n",
    "# Add edges based on OpenAI-determined interactions\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "    \n",
    "    # Query OpenAI for interaction details\n",
    "    reliability_data = ask_openai_for_reliability(guest, statement)\n",
    "    print(f\"Reliability data for {guest}: {reliability_data}\")\n",
    "\n",
    "    interacted_with = reliability_data[0]\n",
    "    only_seen = reliability_data[1]\n",
    "\n",
    "    # Add edges for \"interacted_with\" with reliability 1.0\n",
    "    for other_guest in interacted_with:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 1.0)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=1.0)\n",
    "    \n",
    "    # Add edges for \"only_seen\" with reliability 0.5\n",
    "    for other_guest in only_seen:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 0.5)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=0.5)\n",
    "\n",
    "# Analyze the graph\n",
    "print(\"\\nNetwork Graph Analysis:\")\n",
    "print(f\"Number of nodes (guests): {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges (interactions): {G.number_of_edges()}\")\n",
    "\n",
    "# Identify isolated nodes (guests with no interactions)\n",
    "isolated_guests = list(nx.isolates(G))\n",
    "print(f\"Isolated guests: {isolated_guests}\")\n",
    "\n",
    "# Visualize the graph with edge weights\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "edges = G.edges(data=True)\n",
    "weights = [edge[2]['weight'] for edge in edges]\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_color='lightblue', edge_color=weights,\n",
    "    edge_cmap=plt.cm.Blues, node_size=2000, font_size=10\n",
    ")\n",
    "plt.title(\"Guest Interaction Network with Reliability Parameter\")\n",
    "#plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Blues), label=\"Reliability\")\n",
    "plt.show()\n",
    "\n",
    "# Exclude suspects with high-reliability links\n",
    "possible_suspects = set(G.nodes)\n",
    "for edge in G.edges(data=True):\n",
    "    if edge[2]['weight'] == 1.0:  # High-reliability link\n",
    "        possible_suspects.discard(edge[0])\n",
    "        possible_suspects.discard(edge[1])\n",
    "\n",
    "print(f\"\\nPossible suspects after excluding high-reliability links: {possible_suspects}\")\n",
    "\n",
    "# Compute embeddings for the victim's note\n",
    "victim_note_embedding = model.encode(victim_note, convert_to_tensor=True)\n",
    "\n",
    "# Analyze guest statements for similarity to the victim's note\n",
    "similarities = []\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    if guest in possible_suspects:  # Only consider possible suspects\n",
    "        statement = interrogation['statement']\n",
    "        statement_embedding = model.encode(statement, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(victim_note_embedding, statement_embedding).item()\n",
    "        similarities.append((guest, similarity))\n",
    "\n",
    "# Sort guests by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 3 most similar guests\n",
    "print(\"\\nTop 3 guests with similar writing style to the victim's note:\")\n",
    "for guest, similarity in similarities[:3]:\n",
    "    print(f\"{guest}: Similarity = {similarity:.4f}\")\n",
    "\n",
    "# Combine results to identify the murderer\n",
    "prime_suspect = similarities[0][0]  # Guest with the highest similarity to the victim's note\n",
    "print(f\"\\nPrime Suspect based on writing style: {prime_suspect}\")\n",
    "\n",
    "# Use OpenAI to analyze the note and statements\n",
    "def ask_openai_for_reasoning(victim_note, prime_suspect):\n",
    "    prompt = f\"\"\"\n",
    "    The victim's note is: \"{victim_note}\"\n",
    "    The prime suspect based on writing style is {prime_suspect}.\n",
    "    Explain why this person might be the murderer based on the note and their statement.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "                model=\"openai/gpt-4.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a precise information extraction assistant. Extract only what is explicitly stated.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Get reasoning for the prime suspect\n",
    "reasoning = ask_openai_for_reasoning(victim_note, prime_suspect)\n",
    "print(\"\\nOpenAI Reasoning on the Prime Suspect:\")\n",
    "print(reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b29bf",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Chatgpt isn't responding well to differentiate with guest were only seen and which were interacted with so as a consequence only the list of interacted with is populated and so the after embedding do not work since it takes a void list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60055d",
   "metadata": {},
   "source": [
    "## Code 2\n",
    "\n",
    "I tried to parse the statements myself to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual parser\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "with open('murder_mystery.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "metadata = data['metadata']\n",
    "interrogations = data['interrogations']\n",
    "\n",
    "# Extract the victim's note\n",
    "victim_note = metadata['victim_note']\n",
    "\n",
    "# Initialize the sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def parse_statement_for_reliability(statement):\n",
    "    interacted_with = []\n",
    "    only_seen = []\n",
    "    flag = None  # Keeps track of the current context (interacted_with or only_seen)\n",
    "\n",
    "    # Keywords for interactions and sightings\n",
    "    interaction_keywords = [\"chatting\", \"discussion\", \"talking\", \"playing\"]\n",
    "    sighting_keywords = [\"seen\", \"see\", \"saw\"]\n",
    "\n",
    "    # Split the statement into words\n",
    "    words = statement.split()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "\n",
    "        # Check for interaction keywords\n",
    "        if word in interaction_keywords:\n",
    "            flag = \"interacted_with\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Check for sighting keywords\n",
    "        if word in sighting_keywords:\n",
    "            flag = \"only_seen\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Check for names (two consecutive capitalized words)\n",
    "        if i + 1 < len(words) and words[i][0].isupper() and words[i + 1][0].isupper():\n",
    "            name = f\"{words[i]} {words[i + 1]}\"\n",
    "            if flag == \"interacted_with\" and name not in interacted_with:\n",
    "                interacted_with.append(name)\n",
    "            elif flag == \"only_seen\" and name not in only_seen:\n",
    "                only_seen.append(name)\n",
    "            i += 2  # Skip the second part of the name\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return interacted_with, only_seen\n",
    "# Build a network graph of guest interactions with reliability parameter\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (guests)\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    G.add_node(guest)\n",
    "\n",
    "# Add edges based on parsed interactions\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "\n",
    "    # Parse the statement for reliability\n",
    "    interacted_with, only_seen = parse_statement_for_reliability(statement)\n",
    "    print(f\"Guest: {guest}, Interacted With: {interacted_with}, Only Seen: {only_seen}\")\n",
    "\n",
    "    # Add edges for \"interacted_with\" with reliability 1.0\n",
    "    for other_guest in interacted_with:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 1.0)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=1.0)\n",
    "\n",
    "    # Add edges for \"only_seen\" with reliability 0.5\n",
    "    for other_guest in only_seen:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 0.5)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=0.5)\n",
    "\n",
    "# Analyze the graph\n",
    "print(\"\\nNetwork Graph Analysis:\")\n",
    "print(f\"Number of nodes (guests): {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges (interactions): {G.number_of_edges()}\")\n",
    "\n",
    "# Identify isolated nodes (guests with no interactions)\n",
    "isolated_guests = list(nx.isolates(G))\n",
    "print(f\"Isolated guests: {isolated_guests}\")\n",
    "\n",
    "# Visualize the graph with edge weights\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "edges = G.edges(data=True)\n",
    "weights = [edge[2]['weight'] for edge in edges]\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_color='lightblue', edge_color=weights,\n",
    "    edge_cmap=plt.cm.Blues, node_size=2000, font_size=10\n",
    ")\n",
    "plt.title(\"Guest Interaction Network with Reliability Parameter\")\n",
    "plt.show()\n",
    "\n",
    "# Exclude suspects with high-reliability links\n",
    "possible_suspects = set(G.nodes)\n",
    "for edge in G.edges(data=True):\n",
    "    if edge[2]['weight'] == 1.0:  # High-reliability link\n",
    "        possible_suspects.discard(edge[0])\n",
    "        possible_suspects.discard(edge[1])\n",
    "\n",
    "print(f\"\\nPossible suspects after excluding high-reliability links: {possible_suspects}\")\n",
    "\n",
    "# Compute embeddings for the victim's note\n",
    "victim_note_embedding = model.encode(victim_note, convert_to_tensor=True)\n",
    "\n",
    "# Analyze guest statements for similarity to the victim's note\n",
    "similarities = []\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    if guest in possible_suspects:  # Only consider possible suspects\n",
    "        statement = interrogation['statement']\n",
    "        statement_embedding = model.encode(statement, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(victim_note_embedding, statement_embedding).item()\n",
    "        similarities.append((guest, similarity))\n",
    "\n",
    "# Sort guests by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 3 most similar guests\n",
    "print(\"\\nTop 3 guests with similar writing style to the victim's note:\")\n",
    "for guest, similarity in similarities[:3]:\n",
    "    print(f\"{guest}: Similarity = {similarity:.4f}\")\n",
    "\n",
    "# Combine results to identify the murderer\n",
    "prime_suspect = similarities[0][0]  # Guest with the highest similarity to the victim's note\n",
    "print(f\"\\nPrime Suspect based on writing style: {prime_suspect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a0c83",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "The name are repeated too many time so the graph represented is more messy. Also we have the same problem as before, there are almost no guest that populated the only_seen list, only 'Doctor Ashcroft'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce6fb6",
   "metadata": {},
   "source": [
    "## Code 3\n",
    "\n",
    "I try not to emebed only the suspect list but every guest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "with open('murder_mystery.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "metadata = data['metadata']\n",
    "interrogations = data['interrogations']\n",
    "\n",
    "# Extract the victim's note\n",
    "victim_note = metadata['victim_note']\n",
    "\n",
    "# Initialize the sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to parse statements and assign reliability\n",
    "# def parse_statement_for_reliability(statement):\n",
    "#     interacted_with = []\n",
    "#     only_seen = []\n",
    "\n",
    "#     # Patterns for interactions (reliability = 1.0)\n",
    "#     interaction_keywords = r\"\\b(chatting|discussion|talking|playing)\\b\"\n",
    "#     interaction_pattern = re.compile(f\"{interaction_keywords}.*?([A-Z][a-z]+(?:\\s[A-Z][a-z]+)?)\")\n",
    "\n",
    "#     # Patterns for sightings (reliability = 0.5)\n",
    "#     sighting_keywords = r\"\\b(seen|see|saw)\\b\"\n",
    "#     sighting_pattern = re.compile(f\"{sighting_keywords}.*?([A-Z][a-z]+(?:\\s[A-Z][a-z]+)?)\")\n",
    "\n",
    "#     # Find all matches for interactions\n",
    "#     for match in interaction_pattern.finditer(statement):\n",
    "#         interacted_with.append(match.group(1))\n",
    "\n",
    "#     # Find all matches for sightings\n",
    "#     for match in sighting_pattern.finditer(statement):\n",
    "#         only_seen.append(match.group(1))\n",
    "\n",
    "#     return interacted_with, only_seen\n",
    "# Function to parse statements and assign reliability\n",
    "# Function to parse statements and assign reliability\n",
    "def parse_statement_for_reliability(statement):\n",
    "    interacted_with = []\n",
    "    only_seen = []\n",
    "    flag = None  # Keeps track of the current context (interacted_with or only_seen)\n",
    "\n",
    "    # Keywords for interactions and sightings\n",
    "    interaction_keywords = [\"chatting\", \"discussion\", \"talking\", \"playing\"]\n",
    "    sighting_keywords = [\"seen\", \"see\", \"saw\"]\n",
    "\n",
    "    # Split the statement into words\n",
    "    words = statement.split()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "\n",
    "        # Check for interaction keywords\n",
    "        if word in interaction_keywords:\n",
    "            flag = \"interacted_with\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Check for sighting keywords\n",
    "        if word in sighting_keywords:\n",
    "            flag = \"only_seen\"\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Check for names (two consecutive capitalized words)\n",
    "        if i + 1 < len(words) and words[i][0].isupper() and words[i + 1][0].isupper():\n",
    "            name = f\"{words[i]} {words[i + 1]}\"\n",
    "            if flag == \"interacted_with\" and name not in interacted_with:\n",
    "                interacted_with.append(name)\n",
    "            elif flag == \"only_seen\" and name not in only_seen:\n",
    "                only_seen.append(name)\n",
    "            i += 2  # Skip the second part of the name\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return interacted_with, only_seen\n",
    "# Build a network graph of guest interactions with reliability parameter\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (guests)\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    G.add_node(guest)\n",
    "\n",
    "# Add edges based on parsed interactions\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "\n",
    "    # Parse the statement for reliability\n",
    "    interacted_with, only_seen = parse_statement_for_reliability(statement)\n",
    "    print(f\"Guest: {guest}, Interacted With: {interacted_with}, Only Seen: {only_seen}\")\n",
    "\n",
    "    # Add edges for \"interacted_with\" with reliability 1.0\n",
    "    for other_guest in interacted_with:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 1.0)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=1.0)\n",
    "\n",
    "    # Add edges for \"only_seen\" with reliability 0.5\n",
    "    for other_guest in only_seen:\n",
    "        if G.has_edge(guest, other_guest):\n",
    "            G[guest][other_guest]['weight'] = max(G[guest][other_guest]['weight'], 0.5)\n",
    "        else:\n",
    "            G.add_edge(guest, other_guest, weight=0.5)\n",
    "\n",
    "# Analyze the graph\n",
    "print(\"\\nNetwork Graph Analysis:\")\n",
    "print(f\"Number of nodes (guests): {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges (interactions): {G.number_of_edges()}\")\n",
    "\n",
    "# Identify isolated nodes (guests with no interactions)\n",
    "isolated_guests = list(nx.isolates(G))\n",
    "print(f\"Isolated guests: {isolated_guests}\")\n",
    "\n",
    "# Visualize the graph with edge weights\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "edges = G.edges(data=True)\n",
    "weights = [edge[2]['weight'] for edge in edges]\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_color='lightblue', edge_color=weights,\n",
    "    edge_cmap=plt.cm.Blues, node_size=2000, font_size=10\n",
    ")\n",
    "plt.title(\"Guest Interaction Network with Reliability Parameter\")\n",
    "plt.show()\n",
    "\n",
    "# Exclude suspects with high-reliability links\n",
    "possible_suspects = set(G.nodes)\n",
    "for edge in G.edges(data=True):\n",
    "    if edge[2]['weight'] == 1.0:  # High-reliability link\n",
    "        possible_suspects.discard(edge[0])\n",
    "        possible_suspects.discard(edge[1])\n",
    "\n",
    "print(f\"\\nPossible suspects after excluding high-reliability links: {possible_suspects}\")\n",
    "\n",
    "# Compute embeddings for the victim's note\n",
    "victim_note_embedding = model.encode(victim_note, convert_to_tensor=True)\n",
    "\n",
    "# Analyze guest statements for similarity to the victim's note\n",
    "similarities = []\n",
    "for interrogation in interrogations:\n",
    "    guest = interrogation['guest']\n",
    "    statement = interrogation['statement']\n",
    "    statement_embedding = model.encode(statement, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(victim_note_embedding, statement_embedding).item()\n",
    "    similarities.append((guest, similarity))\n",
    "\n",
    "# Sort guests by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 3 most similar guests\n",
    "print(\"\\nTop 3 guests with similar writing style to the victim's note:\")\n",
    "for guest, similarity in similarities[:3]:\n",
    "    print(f\"{guest}: Similarity = {similarity:.4f}\")\n",
    "\n",
    "# Combine results to identify the murderer\n",
    "prime_suspect = similarities[0][0]  # Guest with the highest similarity to the victim's note\n",
    "print(f\"\\nPrime Suspect based on writing style: {prime_suspect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b3fb4",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "Following my result the most possible murder is Doctor Ashcroft, both based on the network graph since he is the only one that appear in the only_seen list of some guest and so he was not accounted for during the murder and in the graph on code 2 we can see he is more isolated that others. Moreover the analysis made with embedding assign him the higher grade of similarity with the message left on the crime scene.\n",
    "\n",
    "\n",
    "Based on the embeddings other suspect could be Viscount Pemberton and Baron Sienna but they have a reliable link so that means the have alibi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b45c96",
   "metadata": {},
   "source": [
    "###### Note: Code execute on colab "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
